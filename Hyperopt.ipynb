{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /root/anaconda3/lib/python3.11/site-packages (0.2.7)\n",
      "Requirement already satisfied: numpy in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (1.24.3)\n",
      "Requirement already satisfied: scipy in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (1.10.1)\n",
      "Requirement already satisfied: six in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (3.1)\n",
      "Requirement already satisfied: future in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (0.18.3)\n",
      "Requirement already satisfied: tqdm in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (2.2.1)\n",
      "Requirement already satisfied: py4j in /root/anaconda3/lib/python3.11/site-packages (from hyperopt) (0.10.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация клиента\n",
    "# print('Инициализация клиента...')\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url='http://localhost:9000',\n",
    "                  aws_access_key_id='minio',\n",
    "                  aws_secret_access_key='minio123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считывание данных\n",
    "# print('Считывание данных...')\n",
    "obj = s3.get_object(Bucket='datasets', Key='kinopoisk_train.csv')\n",
    "data = obj['Body'].read().decode('utf-8')\n",
    "df = pd.read_csv(StringIO(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('kinopoisk_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка переменных окружения в Unix-подобных системах (Mac, Linux)\n",
    "os.system('export MLFLOW_TRACKING_URI=http://localhost:5000')\n",
    "os.system('export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000')\n",
    "\n",
    "# Установка переменных окружения в Windows\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'http://localhost:5000'\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алгоритмы (пайплайны)\n",
    "algs = dict()\n",
    "\n",
    "algs['text_clf_v_logreg'] = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logreg', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# algs['text_clf_v_mnnb'] = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "#     ('mnnb', SGDClassifier(random_state=42)),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = dict()\n",
    "\n",
    "for model_name, alg in algs.items():\n",
    "    default_params[model_name] = alg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение text_clf_v_logreg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Обучение без параметров (тест)\n",
    "models = dict()\n",
    "\n",
    "for model_name, alg in algs.items():\n",
    "    print(f'Обучение {model_name}...')\n",
    "    models[model_name] = alg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[text_clf_v_mnnb] = models[text_clf_v_mnnb].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели text_clf_v_logreg: 0.7327258921791951\n"
     ]
    }
   ],
   "source": [
    "# Точность без параметров\n",
    "accuracy = dict()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy[model_name] = accuracy_score(y_test, y_pred)\n",
    "    print(f'Точность модели {model_name}:', accuracy[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распаковка словаря\n",
    "def f_unpack_dict(dct):\n",
    "    res = {}\n",
    "    for (k, v) in dct.items():\n",
    "        if isinstance(v, dict):\n",
    "            res = {**res, **f_unpack_dict(v)}\n",
    "        else:\n",
    "            res[k] = v\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание функций для оптимизации\n",
    "objectives = dict()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    def objective(params):\n",
    "        params = f_unpack_dict(params)\n",
    "        print(params)\n",
    "\n",
    "        model = models[model_name]\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(model.get_params())\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        return {'loss': -accuracy, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    objectives[model_name] = objective\n",
    "\n",
    "del(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск параметров для text_clf_v_logreg...\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'saga', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', penalty='l1', solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', penalty='l1', solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "  0%|          | 0/40 [01:08<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression())], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "  2%|▎         | 1/40 [02:07<45:51, 70.54s/trial, best loss: -0.6714755758035941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l1', 'logreg__solver': 'saga', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', penalty='l1', solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', penalty='l1', solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "  5%|▌         | 2/40 [03:11<40:23, 63.77s/trial, best loss: -0.7327258921791951]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'saga', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l2', 'logreg__solver': 'sag', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'liblinear', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty='l1', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty='l1', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'liblinear', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty='l1', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty='l1', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'sag', 'logreg__class_weight': None} \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 18%|█▊        | 7/40 [05:12<14:00, 25.46s/trial, best loss: -0.7456340167046317]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'newton-cg', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', solver='newton-cg'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', solver='newton-cg'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'newton-cg', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'saga', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', penalty='l1', solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', penalty='l1', solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 22%|██▎       | 9/40 [06:46<15:23, 29.78s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'liblinear', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l2', 'logreg__solver': 'saga', 'logreg__class_weight': None} \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'saga', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', penalty=None, solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', penalty=None, solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 30%|███       | 12/40 [08:13<13:54, 29.82s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression())], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 32%|███▎      | 13/40 [09:09<15:47, 35.10s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'liblinear', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l2', 'logreg__solver': 'liblinear', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l2', 'logreg__solver': 'sag', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'liblinear', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'saga', 'logreg__class_weight': 'balanced'}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(class_weight='balanced', penalty=None, solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(class_weight='balanced', penalty=None, solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': 'balanced', 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 45%|████▌     | 18/40 [11:08<08:24, 22.91s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression())], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l2', 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 48%|████▊     | 19/40 [11:59<10:37, 30.35s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': None, 'logreg__solver': 'sag', 'logreg__class_weight': None}  \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 50%|█████     | 20/40 [12:41<12:07, 36.39s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': None, 'logreg__solver': 'sag', 'logreg__class_weight': None}  \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 52%|█████▎    | 21/40 [13:35<12:02, 38.02s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': None, 'logreg__solver': 'sag', 'logreg__class_weight': None}  \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 55%|█████▌    | 22/40 [14:33<12:54, 43.01s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': None, 'logreg__solver': 'sag', 'logreg__class_weight': None}  \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='sag'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='sag'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'sag', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 57%|█████▊    | 23/40 [15:12<13:27, 47.47s/trial, best loss: -0.7689192609465958]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'newton-cg', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='newton-cg'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='newton-cg'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'newton-cg', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'newton-cg', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='newton-cg'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='newton-cg'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'newton-cg', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'saga', 'logreg__class_weight': None} \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty='l1', solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty='l1', solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'lbfgs', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'lbfgs', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': 'l1', 'logreg__solver': 'liblinear', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty='l1', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty='l1', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "{'logreg__penalty': None, 'logreg__solver': 'saga', 'logreg__class_weight': None} \n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty=None, solver='saga'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty=None, solver='saga'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': None, 'logreg__random_state': None, 'logreg__solver': 'saga', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      " 95%|█████████▌| 38/40 [25:56<01:22, 41.44s/trial, best loss: -0.7699316628701595]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__penalty': 'l1', 'logreg__solver': 'liblinear', 'logreg__class_weight': None}\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('logreg', LogisticRegression(penalty='l1', solver='liblinear'))], 'verbose': False, 'vect': CountVectorizer(), 'tfidf': TfidfTransformer(), 'logreg': LogisticRegression(penalty='l1', solver='liblinear'), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'logreg__C': 1.0, 'logreg__class_weight': None, 'logreg__dual': False, 'logreg__fit_intercept': True, 'logreg__intercept_scaling': 1, 'logreg__l1_ratio': None, 'logreg__max_iter': 100, 'logreg__multi_class': 'auto', 'logreg__n_jobs': None, 'logreg__penalty': 'l1', 'logreg__random_state': None, 'logreg__solver': 'liblinear', 'logreg__tol': 0.0001, 'logreg__verbose': 0, 'logreg__warm_start': False}\n",
      "100%|██████████| 40/40 [26:11<00:00, 39.28s/trial, best loss: -0.7699316628701595]\n"
     ]
    }
   ],
   "source": [
    "# Оптимизация параметров\n",
    "spaces = dict()\n",
    "\n",
    "spaces['text_clf_v_logreg'] = {\n",
    "        'group_by__logreg__penalty': hp.choice('hyper_param_groups',\n",
    "            [\n",
    "                {\n",
    "                    'logreg__penalty': hp.choice('penalty_block1', ['l2']),\n",
    "                    'logreg__solver': hp.choice('solver_block1', ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']), # Исключил: ['newton-cholesky']\n",
    "                    # 'logreg__multi_class':hp.choice('multi_class', ['ovr', 'multinomial']),\n",
    "                },\n",
    "                {\n",
    "                    'logreg__penalty': hp.choice('penalty_block3', ['l1']),\n",
    "                    'logreg__solver': hp.choice('solver_block3', ['liblinear', 'saga']),\n",
    "                    # 'logreg__multi_class':hp.choice('multi_class_block3', ['ovr', 'multinomial']),\n",
    "                },\n",
    "                {\n",
    "                    'logreg__penalty': hp.choice('penalty_block4', [None]),\n",
    "                    'logreg__solver': hp.choice('solver_block4', ['lbfgs', 'newton-cg', 'sag', 'saga']), # Исключил: ['newton-cholesky']\n",
    "                },\n",
    "            ]),\n",
    "        'logreg__class_weight': hp.choice('class_weight', ['balanced', None]),\n",
    "        # 'logreg__max_iter': hp.choice('max_iter', [100,500]),\n",
    "    }\n",
    "\n",
    "# spaces['text_clf_v_mnnb'] = {\n",
    "#     'mnnb__loss': hp.choice('loss', ['hinge', 'log_loss', 'modified_huber', \n",
    "#                                      'squared_hinge', 'perceptron', 'squared_error', \n",
    "#                                      'huber', 'epsilon_insensitive', \n",
    "#                                      'squared_epsilon_insensitive']),\n",
    "#     'logreg__dual': hp.choice('dual', [True, False]),\n",
    "#     'logreg__C': hp.loguniform(label='C', low=-4*np.log(10), high=2*np.log(10)),\n",
    "#     'logreg__solver': hp.choice('solver', ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'])\n",
    "# }\n",
    "\n",
    "all_trails = dict()\n",
    "\n",
    "for model_name, space in spaces.items():\n",
    "    print(f'Поиск параметров для {model_name}...')\n",
    "    all_trails[model_name] = Trials()\n",
    "    best = fmin(\n",
    "        fn=objectives[model_name], \n",
    "        space=spaces[model_name], \n",
    "        algo=tpe.suggest, \n",
    "        max_evals=40,\n",
    "        trials=all_trails[model_name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая модель: text_clf_v_logreg\n",
      "Точность: 0.7699316628701595\n"
     ]
    }
   ],
   "source": [
    "best_model_name = min(all_trails.items(), key=lambda x: float(x[1].best_trial['result']['loss']))[0]\n",
    "best_model_trails = all_trails[best_model_name]\n",
    "\n",
    "print('Лучшая модель:', best_model_name)\n",
    "print('Точность:', -best_model_trails.best_trial['result']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 24,\n",
       " 'spec': None,\n",
       " 'result': {'loss': -0.7699316628701595,\n",
       "  'params': {'logreg__penalty': None,\n",
       "   'logreg__solver': 'lbfgs',\n",
       "   'logreg__class_weight': None},\n",
       "  'status': 'ok'},\n",
       " 'misc': {'tid': 24,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'class_weight': [24],\n",
       "   'hyper_param_groups': [24],\n",
       "   'penalty_block1': [],\n",
       "   'penalty_block3': [],\n",
       "   'penalty_block4': [24],\n",
       "   'solver_block1': [],\n",
       "   'solver_block3': [],\n",
       "   'solver_block4': [24]},\n",
       "  'vals': {'class_weight': [1],\n",
       "   'hyper_param_groups': [2],\n",
       "   'penalty_block1': [],\n",
       "   'penalty_block3': [],\n",
       "   'penalty_block4': [0],\n",
       "   'solver_block1': [],\n",
       "   'solver_block3': [],\n",
       "   'solver_block4': [0]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2023, 10, 23, 18, 1, 57, 760000),\n",
       " 'refresh_time': datetime.datetime(2023, 10, 23, 18, 2, 35, 234000)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_trails.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Баккит \"mlflow\" уже существует\n"
     ]
    }
   ],
   "source": [
    "# Создание баккита \"mlflow\"\n",
    "try:\n",
    "    s3.create_bucket(Bucket='mlflow')\n",
    "    print('Баккит \"mlflow\" создан')\n",
    "except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print('Баккит \"mlflow\" уже существует')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Настройка клиента boto3...\n"
     ]
    }
   ],
   "source": [
    "# Настройка клиента boto3\n",
    "print('Настройка клиента boto3...')\n",
    "boto3.setup_default_session(\n",
    "    aws_access_key_id='minio',\n",
    "    aws_secret_access_key='minio123',\n",
    "    region_name='us-west-1'  # или другой регион, если это применимо\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=['CountVectorizer', 'TfidfTransformer', 'LogisticRegression'])\n"
     ]
    }
   ],
   "source": [
    "description = 'Pipeline(steps=' + str([type(s[1]).__name__ for s in models[best_model_name].steps]) + ')'\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логирование в MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'MyOptimizedModel' already exists. Creating a new version of this model...\n",
      "2023/10/23 21:20:12 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: MyOptimizedModel, version 2\n",
      "Created version '2' of model 'MyOptimizedModel'.\n"
     ]
    }
   ],
   "source": [
    "print('Логирование в MLflow...')\n",
    "description = 'Pipeline(steps=' + str([type(s[1]).__name__ for s in models[best_model_name].steps]) + ')'\n",
    "with mlflow.start_run() as run:\n",
    "    # Логирование параметров и метрик\n",
    "    mlflow.log_param(\"model_type\", description)\n",
    "    mlflow.log_metric(\"accuracy\", -best_model_trails.best_trial['result']['loss'])\n",
    "    \n",
    "    # Логирование модели\n",
    "    mlflow.sklearn.log_model(models[best_model_name], \"model\", registered_model_name=\"MyOptimizedModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запуск Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/23 21:20:54 INFO mlflow.utils.conda: === Creating conda environment mlflow-0b5efcd68adffd1ec96421414cd352b3494f6f6b ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ResolvePackageNotFound: \n",
      "  - hyperopt\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/bin/mlflow\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "             ^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/click/core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/anaconda3/lib/python3.11/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/cli.py\", line 205, in run\n",
      "    projects.run(\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/projects/__init__.py\", line 337, in run\n",
      "    submitted_run_obj = _run(\n",
      "                        ^^^^^\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/projects/__init__.py\", line 105, in _run\n",
      "    submitted_run = backend.run(\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/projects/backend/local.py\", line 174, in run\n",
      "    conda_env = get_or_create_conda_env(project.env_config_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/utils/conda.py\", line 273, in get_or_create_conda_env\n",
      "    return _create_conda_env_func(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/utils/conda.py\", line 112, in _create_conda_env\n",
      "    process._exec_cmd(\n",
      "  File \"/root/.local/lib/python3.11/site-packages/mlflow/utils/process.py\", line 117, in _exec_cmd\n",
      "    raise ShellCommandException.from_completed_process(comp_process)\n",
      "mlflow.utils.process.ShellCommandException: Non-zero exit code: 1\n",
      "Command: ['/root/anaconda3/bin/conda', 'env', 'create', '-n', 'mlflow-0b5efcd68adffd1ec96421414cd352b3494f6f6b', '--file', '/mnt/o/ilya/Skillbox/5SM/ML-intro/skillbox_mlflow_train_hyperopt/environment.yml', '--quiet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Установка переменных окружения в Unix-подобных системах (Mac, Linux)\n",
    "os.system('export MLFLOW_TRACKING_URI=http://localhost:5000')\n",
    "os.system('export MLFLOW_S3_ENDPOINT_URL=http://localhost:9000')\n",
    "\n",
    "# Установка переменных окружения в Windows\n",
    "os.environ['MLFLOW_TRACKING_URI'] = 'http://localhost:5000'\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'minio'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'minio123'\n",
    "\n",
    "os.system('mlflow run . --experiment-name=kinopoisk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
